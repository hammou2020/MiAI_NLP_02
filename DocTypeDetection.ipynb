{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DocTypeDetection.ipynb","provenance":[],"authorship_tag":"ABX9TyOh6H98GJClMjTwXeHwM4ix"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"g7QtKvLF7pvk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"6b69375a-032e-42c3-900d-ffa67a88b9f2","executionInfo":{"status":"ok","timestamp":1591775803650,"user_tz":-420,"elapsed":1746,"user":{"displayName":"Thang Nguyen Chien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFW3Qtm_jSkp1xMpwrFqz3y9x-UENOSO4iOlmufg=s64","userId":"12013683752603138284"}}},"source":["import gensim, re\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from os import listdir\n","\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from keras.models import Sequential, load_model\n","from keras.layers import LSTM, Dense, Embedding, GRU, Bidirectional\n","\n","import sys\n","import os\n","\n","# Step 1. Mount drive \n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JZg-Hnme7vWO","colab_type":"code","colab":{}},"source":["%cd /content/gdrive/My\\ Drive\n","# Step 2. Load and make pickle file\n","\n","def txtTokenizer(texts):\n","    tokenizer = Tokenizer()\n","    # fit the tokenizer on our text\n","    tokenizer.fit_on_texts(texts)\n","\n","    # get all words that the tokenizer knows\n","    word_index = tokenizer.word_index\n","    return tokenizer, word_index\n","\n","def preProcess(sentences):\n","\n","    text = [re.sub(r'([^\\s\\w]|_)+', '', sentence) for sentence in sentences if sentence!='']\n","    text = [sentence.lower().strip().split() for sentence in text]\n","    #print(\"Tex=\",text)\n","    return text\n","\n","def loadData(data_folder):\n","\n","    texts = []\n","    labels = []\n","    #\n","    for folder in listdir(data_folder):\n","        #\n","        if folder != \".DS_Store\":\n","            print(\"Load cat: \",folder)\n","            for file in listdir(os.path.join(data_folder , folder)):\n","                #\n","                if file!=\".DS_Store\":\n","                    print(\"Load file: \", file)\n","                    with open(os.path.join(data_folder ,folder , file), 'r', encoding=\"utf-8\") as f:\n","                        all_of_it = f.read()\n","                        sentences  = all_of_it.split('.')\n","\n","                        # Remove garbage\n","                        sentences = preProcess(sentences)\n","\n","                        texts = texts + sentences\n","                        label = [folder for _ in sentences]\n","                        labels = labels + label\n","                        del all_of_it, sentences\n","\n","\n","    return texts, labels\n","\n","data_folder = \"data\"\n","texts, labels = loadData(data_folder)\n","tokenizer, word_index = txtTokenizer(texts)\n","\n","# put the tokens in a matrix\n","X = tokenizer.texts_to_sequences(texts)\n","X = pad_sequences(X)\n","\n","# prepare the labels\n","y = pd.get_dummies(labels)\n","file = open(os.path.join(data_folder ,\"data.pkl\"), 'wb')\n","pickle.dump([X,y, texts],file)\n","file.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7uAKtqTASJH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":564},"outputId":"7ca6ccda-2938-41a5-eb98-2774fe41849e","executionInfo":{"status":"ok","timestamp":1591772343499,"user_tz":-420,"elapsed":1719,"user":{"displayName":"Thang Nguyen Chien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFW3Qtm_jSkp1xMpwrFqz3y9x-UENOSO4iOlmufg=s64","userId":"12013683752603138284"}}},"source":["\n","print(\"After loading raw data\")\n","print(X.shape)\n","print((X[10:30]))\n","print((y[10:30]))\n","print((texts[10:30]))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["After loading raw data\n","(129310, 438)\n","[[   0    0    0 ...   10  120   13]\n"," [   0    0    0 ...   38  110   48]\n"," [   0    0    0 ...  428  110  252]\n"," ...\n"," [   0    0    0 ...   39  496   13]\n"," [   0    0    0 ...  662  581 1249]\n"," [   0    0    0 ...  163    8 2117]]\n","    Economy  Education  Medical\n","10        0          1        0\n","11        0          1        0\n","12        0          1        0\n","13        0          1        0\n","14        0          1        0\n","15        0          1        0\n","16        0          1        0\n","17        0          1        0\n","18        0          1        0\n","19        0          1        0\n","20        0          1        0\n","21        0          1        0\n","22        0          1        0\n","23        0          1        0\n","24        0          1        0\n","25        0          1        0\n","26        0          1        0\n","27        0          1        0\n","28        0          1        0\n","29        0          1        0\n","[['theo', 'hội', 'đồng', 'thi', 'sơn', 'la', 'các', 'phòng', 'bảo', 'quản', 'bài', 'thi', 'chấm', 'thi', 'đều', 'có', 'thiết', 'bị', 'phòng', 'cháy', 'chữa', 'cháy', 'đảm', 'bảo', 'an', 'toàn', 'cho', 'bài', 'thi'], ['các', 'phòng', 'chấm', 'thi', 'đều', 'được', 'lắp', 'camera', 'theo', 'quy', 'định'], ['việc', 'niêm', 'phong', 'các', 'phòng', 'chấm', 'thi', 'bảo', 'quản', 'bài', 'thi', 'hàng', 'ngày', 'thực', 'hiện', 'đúng', 'quy', 'chế'], ['tổng', 'số', 'cán', 'bộ', 'chấm', 'thi', 'của', 'sơn', 'la', 'là', '71', 'người', 'trong', 'đó', '60', 'người', 'chấm', 'thi', 'tự', 'luận'], ['đây', 'là', 'các', 'cán', 'bộ', 'giáo', 'viên', 'trong', 'tỉnh', 'và', '3', 'giảng', 'viên', 'của', 'các', 'trường', 'đh', 'cđ', 'phối', 'hợp'], ['trong', 'khi', 'đó', '11', 'cán', 'bộ', 'giảng', 'viên', 'của', 'đh', 'sư', 'phạm', 'hà', 'nội', '2', 'đang', 'đảm', 'nhận', 'công', 'việc', 'chấm', 'thi', 'trắc', 'nghiệm'], ['để', 'giám', 'sát', 'chặt', 'chẽ', 'điểm', 'nóng', 'này', 'bộ', 'gdđt', 'cử', 'đoàn', 'thanh', 'tra', 'gồm', '3', 'thành', 'viên', 'có', 'mặt', 'tại', 'sơn', 'la', 'từ', 'ngày', '286'], ['trong', 'khi', 'đó', 'đoàn', 'thanh', 'tra', 'do', 'sở', 'gdđt', 'sơn', 'la', 'thành', 'lập', 'gồm', '7', 'thành', 'viên', 'thực', 'hiện', 'nhiệm', 'vụ', 'từ', '306'], ['đến', 'thời', 'điểm', 'này', 'sơn', 'la', 'chấm', 'vòng', '1', 'được', '3'], ['50010'], ['396', 'bài', 'thi', 'ngữ', 'văn', 'chấm', 'vòng', '2', 'được', '500', 'bài'], ['dự', 'kiến', 'đến', 'hết', 'ngày', '77', 'sẽ', 'chấm', 'xong', 'ngày', '107', 'nhập', 'điểm', 'hoàn', 'thiện', 'điểm'], ['18'], ['000', 'bài', 'thi', 'trắc', 'nghiệm', 'được', 'quét', 'và', 'dự', 'kiến', 'đến', 'ngày', '47', 'thì', 'sẽ', 'hoàn', 'tất', 'việc', 'quét', 'bài', 'thi'], ['đối', 'với', 'chấm', 'thi', 'trắc', 'nghiệm', 'kết', 'thúc', 'ngày', 'chấm', 'thi', 'thứ', '2', 'tỉnh', 'đã', 'quét', 'được', '18'], ['000', 'bài'], ['dự', 'kiến', 'hết', 'ngày', '47', 'sẽ', 'quét', 'xong'], ['kiểm', 'tra', 'công', 'tác', 'chấm', 'thi', 'tại', 'sơn', 'la', 'thứ', 'trưởng', 'lê', 'hải', 'an', 'nhắc', 'nhở', 'ban', 'chỉ', 'đạo', 'thi', 'của', 'sơn', 'la', 'phải', 'đảm', 'bảo', 'an', 'ninh', 'an', 'toàn', 'cho', 'tất', 'cả', 'các', 'cán', 'bộ', 'chấm', 'thi'], ['lực', 'lượng', 'thanh', 'tra', 'bộ', 'cần', 'làm', 'đúng', 'quy', 'định', 'của', 'quy', 'chế', 'thi', 'đảm', 'bảo', 'đúng', 'người', 'đúng', 'bài', 'tránh', 'sai', 'sót'], ['hủy', 'bỏ', 'quyết', 'định', 'nghỉ', 'hưu', 'với', 'giám', 'đốc', 'sở', 'gdđt', 'sơn', 'lasơn', 'la', 'hòa', 'bình', 'hà', 'giang', 'chấm', 'thi', 'thpt', 'quốc', 'gia', '2019', 'thế', 'nàonhiều', 'đại', 'học', 'bắt', 'đầu', 'công', 'bố', 'điểm', 'trúng', 'tuyển', 'năm', '2019', 'vtc', 'news', 'dù', 'chưa', 'có', 'điểm', 'thi', 'thpt', 'quốc', 'gia', '2019', 'nhưng', 'đến', 'nay', 'nhiều', 'trường', 'đại', 'học', 'bắt', 'đầu', 'công', 'bố', 'điểm', 'trúng', 'tuyển', 'theo', 'phương', 'thức', 'xét', 'học', 'bạ']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"64RU8cREAmHM","colab_type":"code","colab":{}},"source":["# split in train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\n","\n","# train Word2Vec model on our data\n","word_model = gensim.models.Word2Vec(texts, size=300, min_count=1, iter=10)\n","word_model.save(os.path.join(data_folder,\"word_model.save\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwDKzYRgAs2b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"72d70e39-a9b9-4740-989c-5e42c161dda8","executionInfo":{"status":"ok","timestamp":1591778139405,"user_tz":-420,"elapsed":1991,"user":{"displayName":"Thang Nguyen Chien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFW3Qtm_jSkp1xMpwrFqz3y9x-UENOSO4iOlmufg=s64","userId":"12013683752603138284"}}},"source":["# check the most similar word to 'cơm'\n","print(word_model.wv.most_similar('cơm'))\n","\n","\n","embedding_matrix = np.zeros((len(word_model.wv.vocab) , 300))\n","for i, vec in enumerate(word_model.wv.vectors):\n","  embedding_matrix[i] = vec"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[('cháo', 0.7353115081787109), ('nấu', 0.7341228723526001), ('bún', 0.7034357786178589), ('nướng', 0.6905111074447632), ('xào', 0.6796316504478455), ('mì', 0.6782970428466797), ('phở', 0.6766970157623291), ('nát', 0.6602506637573242), ('rán', 0.6399729251861572), ('chén', 0.6389163732528687)]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"38mh_3LDAu3D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"f18759a2-4666-45dc-f986-508430a90b5e","executionInfo":{"status":"ok","timestamp":1591779350044,"user_tz":-420,"elapsed":1209237,"user":{"displayName":"Thang Nguyen Chien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFW3Qtm_jSkp1xMpwrFqz3y9x-UENOSO4iOlmufg=s64","userId":"12013683752603138284"}}},"source":["# init layer\n","model = Sequential()\n","model.add(Embedding(len(word_model.wv.vocab),300,input_length=X.shape[1],weights=[embedding_matrix],trainable=False))\n","model.add(LSTM(300,return_sequences=False))\n","model.add(Dense(y.shape[1],activation=\"softmax\"))\n","model.summary()\n","model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['acc'])\n","\n","batch = 64\n","epochs = 1\n","model.fit(X_train,y_train,batch,epochs)\n","model.save(os.path.join(data_folder , \"predict_model.save\"))\n","\n","# LSTM - 1188s 10ms/step - loss: 0.3378 - acc: 0.8716"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_10 (Embedding)     (None, 438, 300)          11724600  \n","_________________________________________________________________\n","lstm_4 (LSTM)                (None, 300)               721200    \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 3)                 903       \n","=================================================================\n","Total params: 12,446,703\n","Trainable params: 722,103\n","Non-trainable params: 11,724,600\n","_________________________________________________________________\n","Epoch 1/1\n","116379/116379 [==============================] - 1206s 10ms/step - loss: 0.3392 - acc: 0.8710\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EHDl3mtcJmHs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"56707613-a496-439a-b152-15ff54bfd689","executionInfo":{"status":"ok","timestamp":1591779941179,"user_tz":-420,"elapsed":25092,"user":{"displayName":"Thang Nguyen Chien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFW3Qtm_jSkp1xMpwrFqz3y9x-UENOSO4iOlmufg=s64","userId":"12013683752603138284"}}},"source":["\n","model.evaluate(X_test,y_test)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["12931/12931 [==============================] - 24s 2ms/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.2597367326775022, 0.9033330678939819]"]},"metadata":{"tags":[]},"execution_count":30}]}]}